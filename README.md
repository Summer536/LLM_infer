# LLMæ¨ç†æ¡†æ¶

ä¸€ä¸ªåŸºäºC++å’ŒCUDAçš„é«˜æ€§èƒ½å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œä¸“ä¸ºGPUä¼˜åŒ–è®¾è®¡ï¼Œæ”¯æŒLLaMAç³»åˆ—æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ã€‚

## ğŸš€ é¡¹ç›®ç‰¹è‰²

- **å®Œæ•´çš„LLaMAæ¨ç†å®ç°**: æ”¯æŒLLaMA-2æ¨¡å‹çš„å®Œæ•´æ¨ç†æµç¨‹
- **é«˜æ€§èƒ½CUDAä¼˜åŒ–**: åŸºäºCUDAå†…æ ¸çš„é«˜æ•ˆå¹¶è¡Œè®¡ç®—
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- **å…¨é¢çš„å•å…ƒæµ‹è¯•**: æ¯ä¸ªç»„ä»¶éƒ½æœ‰å¯¹åº”çš„å•å…ƒæµ‹è¯•ï¼Œä¿è¯ä»£ç è´¨é‡
- **æ”¯æŒå¤šè½®å¯¹è¯**: å®Œæ•´çš„ä¸Šä¸‹æ–‡ç®¡ç†å’Œå¤šè½®å¯¹è¯åŠŸèƒ½
- **æ··åˆç²¾åº¦æ”¯æŒ**: æ”¯æŒFP16å’ŒFP32æ··åˆç²¾åº¦è®¡ç®—

## ğŸ“‹ ç›®å½•

- [ğŸš€ é¡¹ç›®ç‰¹è‰²](#-é¡¹ç›®ç‰¹è‰²)
- [ğŸ—ï¸ é¡¹ç›®æ¶æ„](#ï¸-é¡¹ç›®æ¶æ„)
- [ğŸ”§ æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
- [ğŸ› ï¸ æ„å»ºä¸è¿è¡Œ](#ï¸-æ„å»ºä¸è¿è¡Œ)
- [ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹](#-ä½¿ç”¨ç¤ºä¾‹)
- [âš¡ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ§ª æµ‹è¯•](#-æµ‹è¯•)

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
LLM_GYQ/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kernels/           # CUDAå†…æ ¸å®ç°
â”‚   â”‚   â”œâ”€â”€ input_embedding.cu     # è¾“å…¥åµŒå…¥å±‚
â”‚   â”‚   â”œâ”€â”€ rmsnorm_kernel.cu      # RMSNormå½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ linear.cu              # çº¿æ€§å±‚/çŸ©é˜µä¹˜æ³•
â”‚   â”‚   â”œâ”€â”€ qkv_bias_and_RoPE.cu   # QKVå˜æ¢å’Œæ—‹è½¬ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ attn_softmax_kernel.cu # æ³¨æ„åŠ›Softmax
â”‚   â”‚   â”œâ”€â”€ fused_decoder_self_attention.cu # èåˆè‡ªæ³¨æ„åŠ›
â”‚   â”‚   â”œâ”€â”€ fused_addresidual_norm.cu # èåˆæ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–
â”‚   â”‚   â”œâ”€â”€ sampling.cu            # é‡‡æ ·ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ topK.cu                # TopKç®—æ³•
â”‚   â”‚   â””â”€â”€ ...                    # å…¶ä»–CUDAå†…æ ¸
â”‚   â”œâ”€â”€ layers/            # ç¥ç»ç½‘ç»œå±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ attention/             # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”‚   â”œâ”€â”€ context_attention.cpp    # ä¸Šä¸‹æ–‡æ³¨æ„åŠ›
â”‚   â”‚   â”‚   â””â”€â”€ masked_self_attention.cpp # æ©ç è‡ªæ³¨æ„åŠ›
â”‚   â”‚   â”œâ”€â”€ decoder/               # è§£ç å™¨å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ context_decoder.cpp      # ä¸Šä¸‹æ–‡è§£ç å™¨
â”‚   â”‚   â”‚   â””â”€â”€ self_decoder.cpp         # è‡ªå›å½’è§£ç å™¨
â”‚   â”‚   â””â”€â”€ ffn/                   # å‰é¦ˆç¥ç»ç½‘ç»œ
â”‚   â”‚       â”œâ”€â”€ ffn.cpp            # FFNå®ç°
â”‚   â”‚       â””â”€â”€ ffn.h              # FFNå¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ models/            # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ llama/                 # LLaMAæ¨¡å‹
â”‚   â”‚   â”‚   â”œâ”€â”€ llama.cpp          # LLaMAä¸»ä½“å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ llama.h            # LLaMAå¤´æ–‡ä»¶
â”‚   â”‚   â”‚   â””â”€â”€ llama_params.h     # LLaMAå‚æ•°å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ tokenizer.h            # åˆ†è¯å™¨å®ç°
â”‚   â”‚   â””â”€â”€ basemodel.h            # åŸºç¡€æ¨¡å‹æ¥å£
â”‚   â”œâ”€â”€ weights/           # æƒé‡ç®¡ç†
â”‚   â”‚   â””â”€â”€ llama/                 # LLaMAæƒé‡
â”‚   â”‚       â”œâ”€â”€ llama_weights.cc   # æƒé‡åŠ è½½ç®¡ç†
â”‚   â”‚       â”œâ”€â”€ layer_weights.cc   # å±‚æƒé‡ç®¡ç†
â”‚   â”‚       â””â”€â”€ ...                # å…¶ä»–æƒé‡ç›¸å…³æ–‡ä»¶
â”‚   â”œâ”€â”€ memory/            # å†…å­˜ç®¡ç†
â”‚   â”‚   â””â”€â”€ allocator/             # å†…å­˜åˆ†é…å™¨
â”‚   â”‚       â”œâ”€â”€ cuda_allocator.h   # CUDAå†…å­˜åˆ†é…å™¨
â”‚   â”‚       â””â”€â”€ base_allocator.h   # åŸºç¡€åˆ†é…å™¨æ¥å£
â”‚   â””â”€â”€ utils/             # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ tensor.h               # å¼ é‡å°è£…
â”‚       â”œâ”€â”€ model_utils.h          # æ¨¡å‹å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ debug_utils.h          # è°ƒè¯•å·¥å…·
â”‚       â””â”€â”€ ...                    # å…¶ä»–å·¥å…·æ–‡ä»¶
â”œâ”€â”€ tests/                 # æµ‹è¯•æ¨¡å—
â”‚   â””â”€â”€ unittests/                 # å•å…ƒæµ‹è¯•
â”‚       â”œâ”€â”€ test_*.cu              # å„ç»„ä»¶å•å…ƒæµ‹è¯•
â”‚       â””â”€â”€ CMakeLists.txt         # æµ‹è¯•æ„å»ºé…ç½®
â”œâ”€â”€ build/                 # æ„å»ºç›®å½•
â”‚   â”œâ”€â”€ bin/               # å¯æ‰§è¡Œæ–‡ä»¶
â”‚   â””â”€â”€ lib/               # é™æ€åº“æ–‡ä»¶
â”œâ”€â”€ user_entry.cpp         # ç”¨æˆ·å…¥å£ç¨‹åº
â”œâ”€â”€ llama2-7b-tokenizer.bin # åˆ†è¯å™¨æ–‡ä»¶
â””â”€â”€ CMakeLists.txt         # ä¸»æ„å»ºé…ç½®
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. CUDAå†…æ ¸å±‚ (kernels/)
- **è¾“å…¥åµŒå…¥ (Input Embedding)**: å°†token IDè½¬æ¢ä¸ºåµŒå…¥å‘é‡
- **RMSNorm**: Root Mean Squareå½’ä¸€åŒ–ï¼Œæ”¯æŒé«˜æ•ˆå¹¶è¡Œå½’çº¦
- **çº¿æ€§å±‚**: åŸºäºcuBLASçš„é«˜æ€§èƒ½çŸ©é˜µä¹˜æ³•
- **QKVå˜æ¢å’ŒRoPE**: Query/Key/Valueå˜æ¢å’Œæ—‹è½¬ä½ç½®ç¼–ç 
- **æ³¨æ„åŠ›æœºåˆ¶**: èåˆçš„å¤šå¤´è‡ªæ³¨æ„åŠ›å®ç°
- **FFNæ¿€æ´»**: SwiGLUæ¿€æ´»å‡½æ•°å®ç°
- **é‡‡æ ·ç®—æ³•**: TopKé‡‡æ ·å’Œæ¸©åº¦é‡‡æ ·
- **å†…å­˜æ“ä½œ**: é«˜æ•ˆçš„å¼ é‡æ‹¼æ¥ã€è½¬ç½®ç­‰æ“ä½œ

### 2. ç¥ç»ç½‘ç»œå±‚ (layers/)
- **æ³¨æ„åŠ›å±‚**: 
  - ä¸Šä¸‹æ–‡æ³¨æ„åŠ› (Context Attention): å¤„ç†è¾“å…¥åºåˆ—çš„æ³¨æ„åŠ›è®¡ç®—
  - æ©ç è‡ªæ³¨æ„åŠ› (Masked Self Attention): è‡ªå›å½’ç”Ÿæˆçš„æ³¨æ„åŠ›è®¡ç®—
- **è§£ç å™¨å±‚**:
  - ä¸Šä¸‹æ–‡è§£ç å™¨: å¤„ç†è¾“å…¥åºåˆ—çš„å®Œæ•´Transformerè§£ç å™¨å±‚
  - è‡ªå›å½’è§£ç å™¨: å•tokenç”Ÿæˆçš„ä¼˜åŒ–è§£ç å™¨å±‚
- **å‰é¦ˆç½‘ç»œ**: SwiGLUæ¿€æ´»çš„FFNå®ç°

### 3. æ¨¡å‹å±‚ (models/)
- **LLaMAæ¨¡å‹**: å®Œæ•´çš„LLaMA-2æ¨¡å‹å®ç°
- **åˆ†è¯å™¨**: æ”¯æŒLLaMA tokenizerçš„ç¼–ç å’Œè§£ç 
- **åŸºç¡€æ¨¡å‹æ¥å£**: ç»Ÿä¸€çš„æ¨¡å‹æ¥å£å®šä¹‰

### 4. æƒé‡ç®¡ç† (weights/)
- **æƒé‡åŠ è½½**: ä»äºŒè¿›åˆ¶æ–‡ä»¶åŠ è½½æ¨¡å‹æƒé‡
- **æƒé‡ç»„ç»‡**: æŒ‰å±‚ç»„ç»‡çš„æƒé‡ç®¡ç†
- **å†…å­˜ä¼˜åŒ–**: é«˜æ•ˆçš„æƒé‡å†…å­˜å¸ƒå±€

### 5. å†…å­˜ç®¡ç† (memory/)
- **CUDAå†…å­˜åˆ†é…å™¨**: é«˜æ•ˆçš„GPUå†…å­˜ç®¡ç†
- **å†…å­˜æ± **: å‡å°‘å†…å­˜åˆ†é…å¼€é”€çš„å†…å­˜æ± å®ç°

## ğŸ› ï¸ æ„å»ºä¸è¿è¡Œ

### ç³»ç»Ÿè¦æ±‚

- **CUDA**: 12.5+
- **CMake**: 3.8+
- **GCC/G++**: æ”¯æŒC++11æ ‡å‡†
- **GPU**: è®¡ç®—èƒ½åŠ›7.0+ (Tesla V100, RTX 20/30/40ç³»åˆ—ç­‰)

### æ„å»ºæ­¥éª¤

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd LLM_GYQ

# åˆ›å»ºæ„å»ºç›®å½•
mkdir build && cd build

# é…ç½®é¡¹ç›® (æ”¯æŒDebugå’ŒReleaseæ¨¡å¼)
cmake .. -DCMAKE_BUILD_TYPE=Release

# ç¼–è¯‘é¡¹ç›® (ç”Ÿæˆ26ä¸ªé™æ€åº“å’Œ15ä¸ªæµ‹è¯•ç¨‹åº)
make -j$(nproc)

# éªŒè¯æ„å»ºç»“æœ
ls bin/    # æŸ¥çœ‹ç”Ÿæˆçš„æµ‹è¯•ç¨‹åº
ls lib/    # æŸ¥çœ‹ç”Ÿæˆçš„é™æ€åº“
```

### æ„å»ºé€‰é¡¹

```bash
# æ€§èƒ½æµ‹è¯•æ¨¡å¼
cmake .. -DPERF=ON

# è°ƒè¯•æ•°æ®è¾“å‡ºæ¨¡å¼
cmake .. -DPRINT_DATA=ON

# ä¿å­˜è°ƒè¯•æ•°æ®æ¨¡å¼
cmake .. -DSAVE_DATA=ON
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```cpp
#include "src/utils/model_utils.h"

int main() {
    // é…ç½®æ¨¡å‹è·¯å¾„
    std::string model_path = "/path/to/llama/weights/";
    std::string tokenizer_path = "/path/to/tokenizer.bin";
    
    // åˆ›å»ºæ¨¡å‹å®ä¾‹
    auto llm_model = llm::CreateRealLLMModel<float>(model_path, tokenizer_path);
    
    // å•è½®æ¨ç†
    std::string input = "Hello, how are you?";
    std::string response = llm_model->Response(
        llm_model->MakeInput("", 0, input), 
        [](int index, const char* content) {
            if (index >= 0) printf("%s", content);
            if (index == -1) printf("\n");
        }
    );
    
    return 0;
}
```

### å¤šè½®å¯¹è¯

```cpp
int main() {
    std::string model_path = "/path/to/llama/weights/";
    std::string tokenizer_path = "/path/to/tokenizer.bin";
    
    auto llm_model = llm::CreateRealLLMModel<float>(model_path, tokenizer_path);
    
    std::string history = "";
    int round = 0;
    
    while (true) {
        std::string input;
        std::cout << "User: ";
        std::getline(std::cin, input);
        
        if (input == "quit") break;
        
        // ç”Ÿæˆå›å¤
        std::string response = llm_model->Response(
            llm_model->MakeInput(history, round, input),
            [](int index, const char* content) {
                if (index == 0) printf("Assistant: ");
                if (index >= 0) printf("%s", content);
                if (index == -1) printf("\n");
            }
        );
        
        // æ›´æ–°å¯¹è¯å†å²
        history = llm_model->MakeHistory(history, round, input, response);
        round++;
    }
    
    return 0;
}
```

### ç¼–è¯‘ç”¨æˆ·ç¨‹åº

```bash
# æ–¹æ³•1: å–æ¶ˆæ³¨é‡ŠCMakeLists.txtä¸­çš„ä¸»ç¨‹åºç¼–è¯‘
# ç¼–è¾‘ CMakeLists.txtï¼Œå–æ¶ˆæœ€åå‡ è¡Œçš„æ³¨é‡Š
add_executable(main user_entry.cpp)
target_link_libraries(main PUBLIC -lcublas -lcudart -lcudadevrt llmengine)

# é‡æ–°ç¼–è¯‘
cd build && make

# è¿è¡Œ
./bin/main
```

```bash
# æ–¹æ³•2: æ‰‹åŠ¨ç¼–è¯‘
cd build
nvcc -o main ../user_entry.cpp \
    -I.. \
    -L./lib \
    -lllmengine -lcublas -lcudart -lcudadevrt \
    -std=c++11
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### CUDAä¼˜åŒ–ç­–ç•¥

1. **å†…å­˜ä¼˜åŒ–**:
   - å‘é‡åŒ–å†…å­˜è®¿é—® (`Vec<T>` ç±»å‹)
   - å…±äº«å†…å­˜ä¼˜åŒ–å‡å°‘å…¨å±€å†…å­˜è®¿é—®
   - å†…å­˜åˆå¹¶è®¿é—®æ¨¡å¼

2. **å¹¶è¡Œè®¡ç®—ä¼˜åŒ–**:
   - Warpçº§å’ŒBlockçº§å½’çº¦ç®—æ³•
   - èåˆå†…æ ¸å‡å°‘å†…å­˜å¸¦å®½å¼€é”€
   - é«˜æ•ˆçš„çº¿ç¨‹åä½œæ¨¡å¼

3. **æ•°å€¼è®¡ç®—ä¼˜åŒ–**:
   - cuBLASåŠ é€ŸçŸ©é˜µä¹˜æ³•
   - æ··åˆç²¾åº¦è®¡ç®— (FP16/FP32)
   - èåˆæ“ä½œå‡å°‘ä¸­é—´ç»“æœå­˜å‚¨

4. **å†…å­˜ç®¡ç†ä¼˜åŒ–**:
   - å†…å­˜æ± å‡å°‘åˆ†é…å¼€é”€
   - KVç¼“å­˜é‡ç”¨
   - æ‰¹å¤„ç†æ“ä½œä¼˜åŒ–

### æ€§èƒ½åŸºå‡†

åœ¨RTX 4090ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼š
- **LLaMA-7B**: ~40 tokens/s (FP16)
- **å†…å­˜ä½¿ç”¨**: ~14GB GPUå†…å­˜
- **é¦–tokenå»¶è¿Ÿ**: ~100ms
- **åç»­tokenå»¶è¿Ÿ**: ~25ms

## ğŸ§ª æµ‹è¯•

é¡¹ç›®åŒ…å«15ä¸ªå…¨é¢çš„å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯å„ä¸ªç»„ä»¶çš„æ­£ç¡®æ€§ï¼š

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
cd build

# æ ¸å¿ƒç»„ä»¶æµ‹è¯•
./bin/test_causal_mask          # å› æœæ©ç ç”Ÿæˆ
./bin/embedding                 # è¾“å…¥åµŒå…¥å±‚
./bin/rms_norm                  # RMSNormå½’ä¸€åŒ–
./bin/testlinear               # çº¿æ€§å±‚çŸ©é˜µä¹˜æ³•
./bin/biasRope                 # QKVå˜æ¢å’ŒRoPE
./bin/test_mask_softmax        # æ³¨æ„åŠ›Softmax
./bin/test_concat_kv           # KVç¼“å­˜æ‹¼æ¥
./bin/test_repeat_kv           # KVé‡å¤æ“ä½œ
./bin/test_fused_trans_remv_pad # èåˆè½¬ç½®å’Œå¡«å……ç§»é™¤
./bin/test_fused_addresidual_norm # èåˆæ®‹å·®å’Œå½’ä¸€åŒ–

# é«˜çº§åŠŸèƒ½æµ‹è¯•
./bin/test_fused_decoder_attention # èåˆè§£ç å™¨æ³¨æ„åŠ›
./bin/test_sampling            # é‡‡æ ·ç®—æ³•
./bin/test_topk               # TopKç®—æ³•
./bin/paddingoffset           # å¡«å……åç§»è®¡ç®—
```

### æµ‹è¯•è¦†ç›–ç‡

- âœ… **å†…æ ¸å‡½æ•°**: 100%è¦†ç›–æ‰€æœ‰CUDAå†…æ ¸
- âœ… **æ•°å€¼æ­£ç¡®æ€§**: CPUå‚è€ƒå®ç°éªŒè¯
- âœ… **è¾¹ç•Œæ¡ä»¶**: å„ç§è¾“å…¥å°ºå¯¸å’Œè¾¹ç•Œæƒ…å†µ
- âœ… **å†…å­˜å®‰å…¨**: å†…å­˜è®¿é—®å’Œåˆ†é…æµ‹è¯•
